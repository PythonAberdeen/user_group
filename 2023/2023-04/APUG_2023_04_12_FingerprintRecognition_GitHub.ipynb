{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhnhnUC5-SVb"
      },
      "source": [
        "# Fingerprint Matching in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYiM8how-SVg"
      },
      "source": [
        "With this example activity, you will obtain a slight intuition on how biometrics systems work in order to validate a person's fingerprint by using Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AMxeagO7vPV5",
        "outputId": "7258eee3-85c5-4dd9-98fc-36da0c311adc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'user_group'...\n",
            "remote: Enumerating objects: 2746, done.\u001b[K\n",
            "remote: Counting objects: 100% (1219/1219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (704/704), done.\u001b[K\n",
            "remote: Total 2746 (delta 437), reused 1165 (delta 417), pack-reused 1527\u001b[K\n",
            "Receiving objects: 100% (2746/2746), 17.28 MiB | 17.94 MiB/s, done.\n",
            "Resolving deltas: 100% (1084/1084), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PythonAberdeen/user_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQw7ZKPL-SVi"
      },
      "source": [
        "# Importing the Necessary Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF-TLW6S-SVk"
      },
      "source": [
        "To do this activity, we wiil use the following packages:\n",
        "\n",
        "* numpy\n",
        "* matplotlib\n",
        "* scikit image\n",
        "* opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hWHd-1ltRg4"
      },
      "source": [
        "If you are using Google Colab these can be directly imported, otherwise you may need to installing using the `pip install ...` command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sImwyg8O-SVo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import skimage as si\n",
        "import cv2 # to install this package, use pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc0F5aZa-SVp"
      },
      "source": [
        "# Importing and Preprocessing an Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bldUzufs-SVq"
      },
      "source": [
        "We will use four images of fingerprints: Two for subject $101$ (`101_1.tif` and `101_2.tif`) and two for subject $102$ (`102_1.tif` and `102_2.tif`). `.tif` is simply another type of image format, just as `.jpg` or `.png`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSVCvVlztfi_"
      },
      "source": [
        "The first step to create a fingerprint authentication system is to **register** the users into the database. To do so, we will do the following:\n",
        "* Load and \"binarise\" the image of the fingerprint.\n",
        "* Skeletonise the binarised image.\n",
        "* Extract the corner points of the fingerprint ridges.\n",
        "* Extract the SIFT features of the corners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR3G_Or--SVr"
      },
      "source": [
        "## Loading and binarising the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhBrZ76f-SVr"
      },
      "source": [
        "First, we will use the command `cv2.imread` to import the image. This command takes two arguments: the name of the image, and the flag that indicates what type of image to load: 0 for grayscale and 1 for RGB. Usually, biometric images are treated in grayscale. Therefore, we will use the following setting:\n",
        "\n",
        "* `img = cv2.imread(\"IMAGE_FILE\", 0)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6wDq1EBr-SVs"
      },
      "outputs": [],
      "source": [
        "## Use this cell to write the code that will load a fingerprint image\n",
        "img = cv2.imread(\"/content/user_group/2023/2023-04/fingers/101_1.tif\", 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcfXttcP-SVt"
      },
      "source": [
        "Now the image has been imported to Python and has been transformed into a `numpy` array (a matrix of values). If you request to print the variable `img`, you will notice that the system will output an array instead of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK0C5Gx7-SVu"
      },
      "outputs": [],
      "source": [
        "print(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1GkurQq-SVu"
      },
      "source": [
        "Moreover, you can verify the size of the array by using the `.shape` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC80nxPt-SVu"
      },
      "outputs": [],
      "source": [
        "img.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfJRXSW9-SVu"
      },
      "source": [
        "In case you want to see the array as an image, you can use the method \n",
        "\n",
        "* `%matplotlib inline` \n",
        "\n",
        "followed by \n",
        "\n",
        "* `plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh5xigQX-SVv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRcbwlFY-SVv"
      },
      "source": [
        "The image has been imported as an array of values that range from 0 (black) to 255 (white). In between, there are multiple shades of gray that may represent noisy pixels. \n",
        "\n",
        "Therefore, a  process called **binarisation** is applied to the image in order to get rid of this noise and to enhance the shape of the fingerprint. \n",
        "\n",
        "To do so, use the `cv2.threshold()` function:\n",
        "\n",
        "* _, img_bin = cv2.threshold(img, 127, 255, 0)\n",
        "\n",
        "The argumants of the function indicate that all pixels with a value higher than 127 will become 255, and that pixels with a value smaller than 127 will become 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Uz33AdwN-SVw"
      },
      "outputs": [],
      "source": [
        "## Use this cell to write the code that will binarise the fingerprint image\n",
        "ret, img_bin = cv2.threshold(img, 127, 255, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek0BPb2j-SVw"
      },
      "source": [
        "Now you can see that the array only includes values 0 and 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3wyblVS-SVw"
      },
      "outputs": [],
      "source": [
        "img_bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Puq3KD-SVx"
      },
      "source": [
        "Moreover, we can take a look at \"img_bin\" to see how the binarisation afects the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iius6mv3-SVy"
      },
      "outputs": [],
      "source": [
        "plt.imshow(cv2.cvtColor(img_bin, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zBzsvkgB-SVy"
      },
      "source": [
        "## Skeletonisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5f0lVxg-SVz"
      },
      "source": [
        "Now that the image has been binarised, we can apply a filter so that the ridges are thinned and it becomes easier to find the features. To do so, import the **skeletonize** module contained in `scikit-image` morphoology package (`skimage.morphology`):\n",
        "\n",
        "* `from skimage.morphology import skeletonize`\n",
        "\n",
        "Then we convert the 255 into 1 (since the skeletonisation method works only if the matrix contains ones and zeroes):\n",
        "\n",
        "* `img_bin[img_bin == 255] = 1`\n",
        "\n",
        "And finally, we apply the skeletonisation filter and create a new image called `img_skeleton`:\n",
        "\n",
        "* `img_skeleton = np.asarray(skeletonize(img_bin), dtype=np.uint8)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3PRdJ1z3-SVz"
      },
      "outputs": [],
      "source": [
        "## Use this cell to write the code that will skeletonise the binarised fingerprint image\n",
        "from skimage.morphology import skeletonize\n",
        "img_bin[img_bin == 255] = 1\n",
        "img_skeleton = np.asarray(skeletonize(img_bin), dtype=np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_wYFun2-SVz"
      },
      "source": [
        "Let's visualise the image (don't use the second parameter otherwise an all black image will appear):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQu13zxX-SVz"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img_skeleton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTI9tWGx-SVz"
      },
      "source": [
        "## Findig the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELsGLCse-SVz"
      },
      "source": [
        "To find the feature points that describe the fingerprint, we will use two popular methods in image recognition: \n",
        "\n",
        "1. The first one is called Harris corners (which will transform the skeletonised image into a \"Harris corner image\" with indexes describing how likely a pixel is to be a corner).\n",
        "\n",
        "2. The second is Scale Invariant Feature Transform (SIFT) which will extract a vector of descriptor for each of the candidate corners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXIDr69f-SV0"
      },
      "source": [
        "### Harris corners\n",
        "\n",
        "To generate the Harris corners image, use the following commands:\n",
        "\n",
        "* `harris_corners = cv2.cornerHarris(img_skeleton, 3, 3, 0.04)`\n",
        "* `harris_corners_normalized = cv2.normalize(harris_corners, 0, 255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32FC1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z9bT1HrH-SV0"
      },
      "outputs": [],
      "source": [
        "harris_corners = cv2.cornerHarris(img_skeleton, 3, 3, 0.04) \n",
        "harris_corners_norm = cv2.normalize(harris_corners, 0, 255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32FC1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23-C5hWg3mhw"
      },
      "source": [
        "You will get a new matrix with different values than the previous ones; most of them showing $43.802277$ because we are only able to see the corners of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzX-Fw2s-SV0"
      },
      "outputs": [],
      "source": [
        "harris_corners_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhSclYV1-SV2"
      },
      "source": [
        "Notice that if we inspect the Harris corners image, you will see that corners are represented in lighter colours, having a larger value than pixels which are less likely to be corners."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czCpQKJF-SV2"
      },
      "outputs": [],
      "source": [
        "plt.imshow(harris_corners_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "37oTmalj-SV2"
      },
      "source": [
        "### SIFT descriptors\n",
        "\n",
        "To find the *descriptors (features)* for each corner, first we need to establish a **threshold value**. This means that if a pixel in the Harris corners image has a value larger than the threshold, then it is considered a true corner. We will set this threshold empirically as $125$. \n",
        "\n",
        "Then, we will scan the image for all rows and columns to find and extract the keypoints. Finally, we will extract the SIFT descriptors of each keypoint by using the `orb.create()` and the `orb.compute()` functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QwjuWa0x-SV2"
      },
      "outputs": [],
      "source": [
        "threshold_harris = 125\n",
        "keys_101_1 = []\n",
        "for x in range(0, harris_corners_norm.shape[0]):\n",
        "    for y in range(0, harris_corners_norm.shape[1]):\n",
        "        if harris_corners_norm[x][y] > threshold_harris:\n",
        "            keys_101_1.append(cv2.KeyPoint(y, x, 1))\n",
        "orb = cv2.ORB_create()\n",
        "_, desc_101_1 = orb.compute(img, keys_101_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7xsscXm-SV2"
      },
      "source": [
        "You can print the variable `desc` to inspect the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDrmO-oy-SV3"
      },
      "outputs": [],
      "source": [
        "desc_101_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rq3aVRp-SV4"
      },
      "source": [
        "`desc_101_1` should be a numpy array with $1'215$ rows (each representing one true harris corner point) and $32$ columns (each representing a feature). If you are interested to know more about what each of these $32$ features represents, you can read the wikipedia entry for SIFT here:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Scale-invariant_feature_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU4Ep-H5-SV4"
      },
      "outputs": [],
      "source": [
        "desc_101_1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1gCK2XZ-SV4"
      },
      "source": [
        "## Enrolling the Remaining Images\n",
        "\n",
        "Now that the keypoints and descriptors for image `101_1.tif` are stored in variables `keys_101_1` and `desc_101_1` respectively, we will create a function called `enroll()` which you can simply call as many times as you need to enroll the remaining images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sOFapqb9-SV5"
      },
      "outputs": [],
      "source": [
        "## Use this cell to write a function that by being called three times, enrolls the images\n",
        "def enroll(image_name):\n",
        "    '''This function takes the name of the image as input and outputs the keypoints and descriptors.'''\n",
        "    # Step 1: Load the fingerprint image (1 line of code).\n",
        "    img = cv2.imread(image_name, 0)\n",
        "    # Step 2: Binarise the loaded image (1 line of code).\n",
        "    _, img_bin = cv2.threshold(img, 127, 255, 0)\n",
        "    # Step 3: Skeletonise the binarised image (2 lines of code).\n",
        "    img_bin[img_bin == 255] = 1\n",
        "    img_skeleton = np.asarray(skeletonize(img_bin), dtype=np.uint8)\n",
        "    # Step 4: Generate the normalised harris corners image from the skeletonised image (2 lines of code).\n",
        "    harris_corners = cv2.cornerHarris(img_skeleton, 3, 3, 0.04)\n",
        "    harris_corners_norm = cv2.normalize(harris_corners, 0, 255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32FC1)\n",
        "    # Step 5: Extract the SIFT features of the true harris corners with a threshold of 125 (8 lines of code).\n",
        "    threshold_harris = 125\n",
        "    keys = []\n",
        "    for x in range(0, harris_corners_norm.shape[0]):\n",
        "        for y in range(0, harris_corners_norm.shape[1]):\n",
        "            if harris_corners_norm[x][y] > threshold_harris:\n",
        "                keys.append(cv2.KeyPoint(y, x, 1))\n",
        "    orb = cv2.ORB_create()\n",
        "    _, desc = orb.compute(img, keys)\n",
        "    return keys, desc\n",
        "\n",
        "# Call the function three times\n",
        "keys_101_2, desc_101_2 = enroll('/content/user_group/2023/2023-04/fingers/101_2.tif')\n",
        "keys_102_1, desc_102_1 = enroll('/content/user_group/2023/2023-04/fingers/102_1.tif')\n",
        "keys_102_2, desc_102_2 = enroll('/content/user_group/2023/2023-04/fingers/102_2.tif')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXiqUX_f-SV5"
      },
      "source": [
        "## Matching Fingerprints\n",
        "\n",
        "Now that we have all keypoints and descriptors for each fingerprint, we will implement a brute force matching method that will attempt to find the best mapping between a pair of fingerprints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdKB1z5d-SV6"
      },
      "outputs": [],
      "source": [
        "def enroll_and_match(image_nameA, image_nameB):\n",
        "    keysA, descA = enroll(image_nameA)\n",
        "    keysB, descB = enroll(image_nameB)\n",
        "    imgA = cv2.imread(image_nameA, 1)\n",
        "    imgB = cv2.imread(image_nameB, 1)\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    match = sorted(bf.match(descA, descB), key= lambda match:match.distance)\n",
        "    # Plot keypoints\n",
        "    imgA_keys = cv2.drawKeypoints(imgA, keysA, outImage=None)\n",
        "    imgB_keys = cv2.drawKeypoints(imgB, keysB, outImage=None)\n",
        "    f, axarr = plt.subplots(1,2)\n",
        "    axarr[0].imshow(imgA_keys)\n",
        "    axarr[1].imshow(imgB_keys)\n",
        "    plt.show()\n",
        "    # Plot matches\n",
        "    \n",
        "    ## with this code you only plot the matches that are potentially true\n",
        "    plot_match = []\n",
        "    for ma in match:\n",
        "        if ma.distance < 73:\n",
        "            plot_match.append(ma)\n",
        "    \n",
        "    img_match = cv2.drawMatches(imgA, keysA, imgB, keysB, plot_match, flags=2, outImg=None)\n",
        "    plt.imshow(img_match)\n",
        "    plt.show()\n",
        "    return match\n",
        "\n",
        "# Call the function\n",
        "match = enroll_and_match('/content/user_group/2023/2023-04/fingers/101_1.tif',\n",
        "                         '/content/user_group/2023/2023-04/fingers/101_1.tif')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOkT7_aY-SV6"
      },
      "source": [
        "## Evaluate the \"goodness\" of the match\n",
        "\n",
        "As commented before, the key of biometrics is that you can never guarantee an exact match (unless you are comparing the same two images as above), but most of the times you are matching two images and declaring if they belong to the same subject or not based on a score/threshold. \n",
        "\n",
        "This score is calculated based on the distance between one matched feature and the other. Moreover, the score threshold is set empirically based on previous experiments\n",
        "\n",
        "For this example, we will set the acceptance threshold equal to $55$. This means that if the matching obtains this score or lower, then the two fingerprints are of the same person, and if the value is higher, then the fingerprints don't match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR7qPuoe-SV7"
      },
      "outputs": [],
      "source": [
        "def score(match):\n",
        "    score = 0\n",
        "    score_threshold = 55\n",
        "    for ma in match:\n",
        "        score += ma.distance\n",
        "    score = score/len(match)\n",
        "    print('Score is', score)\n",
        "    if score < score_threshold:\n",
        "        print(\"Fingerprints match.\")\n",
        "    else:\n",
        "        print(\"Fingerprints do not match.\")\n",
        "    return\n",
        "\n",
        "## With this code you can run enroll and match sequentially\n",
        "match = enroll_and_match('/content/user_group/2023/2023-04/fingers/101_1.tif', \n",
        "                         '/content/user_group/2023/2023-04/fingers/101_2.tif')\n",
        "score(match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wLTiLrT6i0C"
      },
      "outputs": [],
      "source": [
        "## With this code you can run enroll and match sequentially\n",
        "match = enroll_and_match('/content/user_group/2023/2023-04/fingers/102_1.tif', \n",
        "                         '/content/user_group/2023/2023-04/fingers/102_2.tif')\n",
        "score(match)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## With this code you can run enroll and match sequentially\n",
        "match = enroll_and_match('/content/user_group/2023/2023-04/fingers/101_1.tif', \n",
        "                         '/content/user_group/2023/2023-04/fingers/102_2.tif')\n",
        "score(match)"
      ],
      "metadata": {
        "id": "DxHEMAgr7OHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vlztONV7gid"
      },
      "source": [
        "This rationale applies also for palmprints and iris (eyes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge time!"
      ],
      "metadata": {
        "id": "iwrpYYFrpjtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For beginners\n",
        "* Make sure the system works in your machine/colab account!\n",
        "* Play with the parameters and the data to see how things change\n",
        "* Can you load the data from Github rather than my dropbox?"
      ],
      "metadata": {
        "id": "_pkx1U5ipl26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For intermediates\n",
        "* Adapt the code so that it works with palmprints\n",
        "  * You can get some images from here: https://www.dropbox.com/s/1ckcravisjwu31k/palms.zip?raw=1 or our GitHub!\n",
        "* Change the Orb feature extractor to [SURF](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html) and see if it's any simpler/better"
      ],
      "metadata": {
        "id": "fEldizLSp7ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Experts\n",
        "* As you have seen, we need to manually set the threshold to determine if there's a match!\n",
        "  * Could this be done differently?\n",
        "* Can you apply **STRUCTURAL** matching in the fingerprints or palms? The salient points detected are connected between them, and if we are able to understand these connections, then our matching algorithm would be more effective! Try to use [Delaunay triangulation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Delaunay.html) to generate a **graph** from the points. Afterwards, you can use [NetworkX Matching](https://networkx.org/documentation/stable/reference/algorithms/matching.html) or [GMatch4py](https://github.com/Jacobe2169/GMatch4py) to match the graphs."
      ],
      "metadata": {
        "id": "7GKiOYMPp98c"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}